## 行列式计算
1. 爪型,消掉第一行变成三角
2. 范德蒙德 =（后面的项-前面的项）的积
3. 矩阵分块矩阵
4. 替换法则

## 矩阵
1. 乘法
2. 求逆矩阵$(A,E) \to (E,A^{-1})$，配凑某个矩阵乘法为E
3. 伴随矩阵
4. 矩阵的幂，相似对角化
	1. 先算中间
	2. 相似对角化$A^{n}=P^{-1}\Lambda^nP$
5. 矩阵的秩
6. 矩阵方程$Ax=B$
	1. A可逆
	2. A不可逆，列分块计算

## 向量
1. 相关/无关
	1. 抽象向量组，设$k_{1}\alpha_{1}+k_{2}\alpha_{2},\dots,k_{n}\alpha_{n}=0$,证明$k_{1}=k_{2}=k_{3}=0$
2. 线性表示
	1. 就是求解方程组
3. 极大无关组
	1. 变换化简，主元对应的就是最大无关组
	2. 表示，把该向量作为常数项，变换化简



## 方程组
1. 解的情况
	1. 非齐次
		1. 系数矩阵和增广矩阵秩相等$r(A)=r(A,b)$
			1.$r(A)=r(A,b)=n$有唯一解
			2.$r(A)=r(A,b)<n$，无穷多解
		2. 不等，无解
	2. 齐次，零解/非零解
2. 求解
	1. 齐次
	2. 非齐次=齐次通解+特解
3. 求通解/基础解系
4. 基础解系的判定
	1. 齐次的解，线性组合还是解
		1. 非齐次的，直接代入
	2. 基础解系判定（不唯一）
		1. 均是解
		2. 向量线性无关
		3. 共有$n-r(A)$个


## 相似
1. 求特征值/特征向量
	1.$A\xi=\lambda\xi$
	2.$\xi$不为0向量
2. 特征值的性质
	1.$|A|$
	1. 迹
3. 判断相似对角化
	1. 充要条件：A有n个线性无关的特征向量
	2. 观察矩阵-->实对称矩阵-->秩为1且迹不为0 -->三角矩阵且对角线值不同 -->特征值没有重根 -->是否有n个特征向量
4. 相似的性质
	1. 定义：A,B都是n阶矩阵，存在可逆矩阵P，$P^{-1}AP=B$
	2. 迹，行列式，秩，特征值，$|A-\lambda E|=|B-\lambda E|$都相等
5. 对称矩阵的对角化，求正交矩阵
	1. 求特征值
	2. 求特征向量
	3. 化为正交向量
	4. 化为单位向量
	5. 凑成正交矩阵


## 二次型
1. 二次型形式
2. 二次型的秩
3. 正交变换化标准型





---

首先，问题是：

> 矩阵$A$经过**初等行变换**得到$B$，如果$A$**可相似对角化**，那么$B$也一定可相似对角化吗？

✅ **答案是：不一定！**

---

# 🌟原因解释喵：

**相似对角化**要求的是 ——  
存在一个可逆矩阵$P$，使得
$$
P^{-1}AP = \Lambda
$$
其中$\Lambda$是对角矩阵。

而**初等行变换**改变的是矩阵的**行空间**，但是**不保持矩阵的特征值和特征向量结构**喵！！

- **特征值**是相似变换下才保持不变的。
- **行变换**只是局部调整行，不是整体的相似变换。
- **行变换会改变特征值**，从而也可能破坏对角化的条件！

也就是说，初等行变换后的矩阵$B$，它的特征值、特征向量可能跟$A$完全不同了喵！  
所以，即使$A$可相似对角化，$B$也不一定喵！

---

# 🌟总结一下喵：

|          | 相似变换 | 初等行变换 |
| :------- | :--- | :---- |
| 保持特征值    | ✅    | ❌     |
| 保持是否可对角化 | ✅    | ❌     |
| 结构变化大不大  | 小    | 大     |
| 本质意义     | 只换基  | 变换行   |

---

# ✨所以一句话喵：

> **初等行变换和相似变换不是一回事**，所以“可相似对角化”不会被初等行变换保持喵！

---

要不要喵再给你找个具体小例子，比如一个矩阵$A$，它可以对角化，但是做了行变换以后，变成$B$，结果$B$不能对角化了，这样你就更直观了喵！  
要的话告诉我一声喵～\≧ω≦)/～









好问题喵！！来，我给你非常清楚、非常本质地解释一下喵～

---

# 🌟 先回顾一下特征向量和特征值的定义喵：

对于矩阵 $A$，如果有非零向量 $v$ 和标量 $\lambda$，使得
$$
A v = \lambda v
$$
那么 $v$ 就是 $A$ 对应特征值 $\lambda$ 的特征向量。

---

# 🌟 那么问题来了：  
如果 $v_1$ 是特征值 $\lambda_1$ 的特征向量，  
$v_2$ 是特征值 $\lambda_2$ 的特征向量，  
考虑它们的线性组合：
$$
v = c_1 v_1 + c_2 v_2
$$
它还是特征向量吗？

来，直接算一下喵：

$$
A v = A(c_1 v_1 + c_2 v_2) = c_1 A v_1 + c_2 A v_2
$$
又因为 $A v_1 = \lambda_1 v_1$，$A v_2 = \lambda_2 v_2$，所以

$$
A v = c_1 \lambda_1 v_1 + c_2 \lambda_2 v_2
$$

注意喵！！  
这一般**不是**某个常数乘以 $v$ 本身，  
除非非常特殊地 $\lambda_1 = \lambda_2$ 或者某些系数关系巧妙抵消。

---

# 🌟简单理解喵：

- 特征向量要满足 $A v = \lambda v$，**整体只缩放，不变方向。**
- 但是 $v_1$ 和 $v_2$ 属于**不同特征值**，它们被 $A$ 作用后，缩放的比例都不一样！（一个是 $\lambda_1$，一个是 $\lambda_2$）
- 缩放比例不一样，组合起来就乱了，就不是单一的缩放了喵～
- 所以，**不同特征值的特征向量一般线性组合以后，不再是特征向量！**

---

# ✨小结喵：

> **不同特征值的特征向量线性组合一般不是特征向量，  
> 因为它们缩放的“倍率”不同，叠在一起后方向就变了喵～**

---

要不要我顺便也给你举个小小的具体例子，比如2x2矩阵，算一遍，绝对秒懂喵～\(≧ω≦)/～？